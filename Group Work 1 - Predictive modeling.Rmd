---
title: "Group Work 1 - Predictive modeling"
author: "Donjet Dzemaili, Fereshteh Ahmadi, Michael Etter, Seraina Zimmermann"
date: "2025-05-26"
output: html_document
---

# Introduction
The purpose of this project is to develop a predictive model that estimates individual wages based on a wide range of personal, educational, and professional characteristics. Using the dataset data_wage.RData, our goal is to apply supervised learning techniques to accurately predict the variable of interest, which is the wage.

Beyond predictive accuracy, a key focus of this project is on Explainable Artificial Intelligence (XAI). While machine learning models can uncover complex patterns and deliver powerful predictions, they often operate as black boxes. This raises critical concerns about transparency, fairness, and trust. XAI methods help address these challenges by making model behavior interpretable and understandable. In this project, we apply explainability techniques to identify the key factors driving wage predictions.

Through this approach, we aim not only to build a high-performing model but also to ensure that its predictions are transparent, fair, and justifiable.

We will guide you through the entire process, starting with data exploration and cleaning, followed by model training and testing, and finally using the model to estimate the future salaries of our group members.

# Load and prepare the data
We start by loading the original dataset data_wage.RData. To ensure a clear workflow, we rename the dataset to data_original and remove the old variable name to avoid confusion in the following steps.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load libraries
library(writexl)
library(readxl)
library(knitr)
library(dplyr)
library(tibble)
library(ggplot2)

# Load the data
load("data_wage.RData")

# Rename 
data_original <- data
rm(data)  # Remove old name to avoid confusion

```

Since one objective of this project is to use our predictive model to estimate the future wages of our own group members, we will export the original dataset into an editable Excel file. Each team member will then manually create a new row by answering the same set of questions included in the dataset, based on their personal background, education, experience, and other relevant characteristics.

This method allows us to maintain consistency with the dataset’s structure and ensures that our self-assessed entries are in the correct format. 

```{r export csv file to data}

# Export to Excel
write_xlsx(data_original, "data_wage_editable.xlsx")

```

Now that all team members have completed their entries, we re-imported the updated Excel file. These additional rows are now part of the dataset and will be included in our analysis and model development moving forward.

```{r upload filled excel file}

# Read the filled-in Excel file
data <- read_excel("data_wage_filled.xlsx")

```

# Data Exploration
We begin our analysis with an initial exploration of the dataset to understand its structure and contents. This step is essential to get a sense of the data we're working with and to identify any immediate issues that may need attention.

## Dimensions of the dataset
As a first step, we examine the dimensions of the dataset data, which now includes both the original survey responses and the additional entries created by our team members. Understanding the number of observations and variables provides a foundation for all further analysis.

```{r dimensions }

dim(data)

```

The dataset consists of 10,813 observations and 78 variables. This includes the original 10,809 responses from the survey as well as 4 additional rows that were added manually by our team members. Each observation represents one individual and the variables capture various demographic, educational, and professional characteristics relevant to wage prediction.

## First and last few rows
To get a sense of the structure and format of our dataset, we display the first and last few rows. Given that the dataset includes 78 variables, we only show a small selection of rows and columns to maintain readability.

```{r head}

# Show first 5 rows and first 10 variables
kable(
  head(data[, 1:10], 5),
  caption = "First 5 Rows (First 10 Variables)"
)

```

Looking at the first few rows, some striking patterns immediately jump out — and not by coincidence. The first four entries all come from Switzerland, represent the 22–24 age range, and hold a Bachelor's degree in Mathematics or Statistics. They’re either aspiring Data Analysts or Data Scientists, and every one of them is a student with 0–1 years of experience. 

In short, it’s not hard to guess: these are us. 

Further we can notice that the last column shown in this preview is a one-hot encoded variable that indicates whether someone is involved in this specific ML-related activity.

```{r tail}

# Show last 5rows and first 10 variables
kable(
  tail(data[, 1:10], 5),
  caption = "Last 5 Rows (First 10 Variables)"
)


```

At the other end of the dataset, the last five rows bring us back to the diversity of the original survey respondents. Here we see individuals from countries like Turkey, India, Russia, the Netherlands, and France. Their backgrounds are just as varied — ranging from fresh graduates to seasoned professionals with over 20 years of experience. This variety highlights the richness of the dataset and the breadth of real-world contexts it captures — which will be crucial for building a model that generalizes well.

## Structure of the dataset
To better understand the makeup of our dataset, we begin by inspecting the structure of the variables. With 78 variables in total, a full printout using str(data) would be too lengthy and clutter the markdown output. Instead, we limit the preview to the first 9 variables, which gives us a clear sense of how the dataset is structured without overwhelming the document.

```{r structure }

str(data[, 1:9])

```

The structure preview of the first 10 variables reveals that all of them are currently stored as character data types (chr). This is important to note because many of these variables — such as gender, education, and job_role — are clearly categorical in nature. Before we can use them in a predictive model, we will need to convert them into appropriate formats, such as factors or one-hot encoded variables. This observation highlights the need for a careful and thoughtful preprocessing phase, especially for handling categorical data correctly.

In addition to this partial structure, we summarize the data types of all variables in the dataset. This overview helps us identify how many variables are categorical, numeric, logical, or of other types.

```{r structure }

# Count variable types
table(sapply(data, class))

```

This output tells us that the dataset contains 14 character variables and 64 numeric variables. The high number of numeric variables suggests that many features are already in a format suitable for modeling, which is convenient. However, the 14 character variables will require transformation. These are likely to include key predictors such as demographic information, education level, job role, or region — all of which may play an important role in wage prediction.

This breakdown gives us a solid foundation for planning our preprocessing pipeline in the next steps.

# Data Quality Issues 
Before diving into feature selection and model building, it is essential to assess the quality of the dataset. High-quality data is the foundation of any reliable predictive model. In this section, we systematically check for common data quality issues such as missing values, inconsistent formatting, or variable types that may require transformation.

Identifying and addressing these issues early on ensures that the subsequent steps are built on clean and trustworthy data

## Completeness
One of the first aspects to assess when evaluating data quality is completeness. Specifically, whether there are any missing values (NAs) in the dataset. Missing values can negatively impact the performance of machine learning models and must be handled appropriately before training.

To investigate completeness, we inspect the dataset for the presence of missing values.

```{r completeness }

# Total number of missing values in the dataset
sum(is.na(data))

```

The dataset contains 4 missing values. Let’s identify which variable they belong to.

```{r completeness - number of missing values }

# Number of missing values per variable
na_counts <- colSums(is.na(data))
na_counts[na_counts > 0]  # Show only variables with missing values

```

These missing values are all located in the wage variable and correspond to the four manually added rows from our group members. We intentionally left the wage field blank, as these entries are meant to serve as test cases for our final model predictions. This makes perfect sense from a project perspective.

**What to Do with These NA Wages?**
To ensure a clean and consistent dataset for model training, we extract all observations where the wage value is missing (NA) into a new dataset named data_team. These rows represent the entries from our team members and are excluded from the training data.

After this separation, we remove these rows from the main dataset (data). The result is a dataset that contains only the original survey responses, all of which are complete and contain no missing values. This allows us to proceed with confidence, knowing that our training data is fully intact and suitable for building a reliable predictive model.

```{r completeness - number of missing values }

# Extract rows with NA in wage (our own entries)
data_team <- data %>% filter(is.na(wage))

# Remove these rows from the main dataset
data <- data %>% filter(!is.na(wage))

```

## Duplicates
Another important step in assessing data quality is checking for duplicate rows. Duplicate entries can bias model training by overrepresenting certain individuals or responses, potentially skewing the predictions. Especially in survey data, it’s important to ensure each response represents a unique individual.

In this section, we check whether the dataset contains any exact duplicates.

```{r duplicates }

# Check for duplicate rows
sum(duplicated(data))

```

The output shows that there are 0 duplicate rows in the dataset. This confirms that each observation in the dataset is unique, and no duplicate responses are present. 

## Data Consistency
Before we can train any reliable machine learning model, we need to ensure that the data is not only complete but also internally consistent. Data consistency refers to making sure that each variable is stored in the correct format, contains coherent values, and is structured in a way that models can properly interpret.

In this step, we focus on aligning data types with their intended use  and checking for inconsistencies in how categories are recorded. 

Ensuring consistency at this stage prevents problems during model training and improves both the performance and interpretability of our models.

### Convert Categorical Variables to Factors
In this step, we ensure that all categorical variables in the dataset are consistently and correctly formatted by converting them from character type to factor type. This is important because many of our variables — such as gender, education, or job role — represent distinct categories rather than free text. 

In R, treating these variables as factors allows machine learning models to correctly interpret them as discrete, non-numeric values with specific levels. This distinction is essential for algorithms like decision trees or random forests, which can leverage the factor structure directly. 

We perform this transformation using a simple and efficient command that automatically converts all character variables in the dataset into factors.

```{r convert categorical variables to factors}

# Convert all character variables to factors
data <- data %>%
  mutate(across(where(is.character), as.factor))

# Count variable types
table(sapply(data, class))

```

All categorical variables have now been successfully converted into factors, ensuring that they are properly recognized and handled as discrete categories in the modeling process.

### Standardize inconsistent values
Before moving on to modeling, we take a moment to ensure that all factor levels are consistent and free from redundant or misformatted entries. In survey data, it's common to encounter small inconsistencies — for example, the same category entered with different capitalizations or spellings.

```{r standardize }

# Extract factor variables
factor_vars <- data[, sapply(data, is.factor)]

# Create a list of factor levels
levels_list <- lapply(factor_vars, levels)

# Convert to a named data frame for display
levels_table <- tibble(
  Variable = names(levels_list),
  Levels = sapply(levels_list, function(x) paste(x, collapse = ", "))
)

# Display as a kable table
kable(levels_table, caption = "Levels of All Factor Variables (After Standardization)")

```

After inspecting the levels of all factor variables, we found that the only noteworthy inconsistency appears in the variable Programming_language_used_most_often, where both python and Python exist as separate categories. Since these clearly refer to the same programming language, we standardize them by replacing all instances of python with Python. This ensures consistency and prevents the model from treating identical values as different categories. 

No other inconsistencies were detected in the factor levels.

```{r replace python with Python }


# Replace 'python' with 'Python'
levels(data$Programming_language_used_most_often)[
  levels(data$Programming_language_used_most_often) == "python"
] <- "Python"

# Check updated levels
levels(data$Programming_language_used_most_often)


```
As we can now see, only Python remains, which confirms that the the inconsistency has been resolved.

## Outliers
Outliers are data points that deviate significantly from the overall distribution of a variable. For instance, an individual reporting 40 years of work experience in a dataset where the majority have less than 10 years, or a salary that is ten times higher than the average, would be considered outliers. These extreme values can distort model estimates, influence summary statistics, and influence model performance, especially in regression-based methods. Therefore, it is crucial to detect and assess such values before proceeding with model development.

Since our target variable is wage, this is the primary variable of concern when checking for outliers. We begin by visualizing the wage distribution to detect any extreme values.

```{r outliers wage plot }

# Boxplot showing wage distribution with outliers
ggplot(data, aes(y = wage)) +
  geom_boxplot(fill = "lightblue", outlier.shape = 16) +
  labs(
    title = "Boxplot of Wage",
    y = "Wage (USD)"
  )


```
We can see that we do have some outliers in the wage variable, as expected. Let’s now examine how many observations fall outside the typical range.

To identify outliers, we use the Interquartile Range (IQR) rule, a standard method for detecting extreme values in continuous data. The IQR is the difference between the third quartile (Q3) and the first quartile (Q1). Outliers are defined as values that fall below Q1 - 1.5 × IQR or above Q3 + 1.5 × IQR. The table below shows how many observations in our dataset meet this condition. TRUE indicates an observation is classified as an outlier. FALSE means the observation lies within the normal range and is not an outlier.

```{r outliers wage table }

# Calculate IQR thresholds
Q1 <- quantile(data$wage, 0.25)
Q3 <- quantile(data$wage, 0.75)
IQR_value <- Q3 - Q1

lower_bound <- Q1 - 1.5 * IQR_value
upper_bound <- Q3 + 1.5 * IQR_value

# Flag outliers
data$outlier_flag <- ifelse(data$wage < lower_bound | data$wage > upper_bound, TRUE, FALSE)

# Count outliers vs non-outliers
table(data$outlier_flag)


```

Based on the IQR rule, a total of 275 observations have been identified as outliers in the wage variable. Given that this number is relatively small compared to the 10,534 non-outlier observations, we decide to remove these outliers from the dataset. This approach ensures that our model is trained on clean and representative data, without being influenced by extreme wage values that could distort predictions.

```{r outliers wage remove }

# Remove wage outliers from the dataset
data <- data %>% filter(outlier_flag == FALSE)

# Count outliers vs non-outliers
table(data$outlier_flag)

#Remove the outlier_flag column
data$outlier_flag <- NULL


```
As shown in the table above, all outliers have been successfully removed from the dataset. The only remaining entries are those labeled FALSE, confirming that no outliers are present in the wage variable anymore.

As the other variables in the dataset are all either of type factor or numeric, no further outlier treatment is necessary. Outlier detection is only relevant for continuous numeric variables. All factor variables represent categorical information and therefore do not require outlier handling. The remaining numeric variables are exclusively one-hot encoded (0/1) indicators derived from multiple-choice survey questions. Since these variables are binary by nature, they cannot contain outliers and are excluded from further outlier analysis.

## Investigate the target variable
Before building our model, we explore the distribution and characteristics of the target variable wage. This helps us understand its range, detect anomalies, and ensure it's suitable for predictive modeling.

The summary provides basic descriptive statistics of the wage variable, including the minimum, maximum, mean, median, and quartiles. It helps identify unusual values and assess the scale and spread of the variable.

```{r summary wage}

summary(data$wage)

```

The histogram, overlaid with a density curve, shows the overall distribution of wage values in the dataset. It allows us to visually assess skewness and the presence of any prominent peaks or gaps.

```{r histogram wage }


ggplot(data, aes(x = wage)) +
  geom_histogram(binwidth = 5000, fill = "steelblue", color = "white") +
  geom_density(aes(y = ..count.. * 5000), color = "red", linewidth = 1) +
  labs(title = "Distribution of Wage", x = "Wage (USD)", y = "Count")

```

After inspecting the distribution, we observe that a number of respondents have a wage value of 0. These entries do not provide meaningful target values for model training, as no income is recorded. Since our goal is to predict actual wage values, we remove these cases from the dataset.

```{r remove wage equal 0}

# Remove observations where wage equals 0
data <- data %>% filter(wage > 0)


```


## Standardization
In our dataset, all features are either categorical variables stored as factors or already one-hot encoded numeric variables (binary 0/1 indicators). Therefore, feature scaling or standardization is not required.

Standardization is typically applied to continuous numeric variables to ensure they are on a comparable scale, especially when using models that are sensitive to feature magnitude (e.g., linear regression, support vector machines, or K-nearest neighbors). However, this is not the case in our setup.

Furthermore, since we plan to use tree-based algorithms for prediction (such as decision trees, random forests, or gradient boosting), standardization would have no effect on model performance. 

As a result, we proceed without applying any feature scaling, as all variables are already in a format suitable for modeling.

# Correlation Matrix
Clear justification for selected features. why did we chose the features we have chosen

```{r correlation matrix}

# Load required libraries
library(dplyr)
library(corrplot)

# Subset only numeric variables (one-hot encoded features)
numeric_data <- data %>% select(where(is.numeric))

# Compute the correlation matrix
cor_matrix <- cor(numeric_data, use = "pairwise.complete.obs")

# Extract only pairs with moderate to high correlation (> 0.5 and < 1)
high_corr <- which(abs(cor_matrix) > 0.5 & abs(cor_matrix) < 1, arr.ind = TRUE)

# Get the unique variable names involved
high_corr_vars <- unique(c(rownames(cor_matrix)[high_corr[,1]], colnames(cor_matrix)[high_corr[,2]]))

# Filter matrix to those variables only
cor_matrix_filtered <- cor_matrix[high_corr_vars, high_corr_vars]

# Plot filtered matrix with coefficients
if (length(high_corr_vars) >= 2) {
  corrplot(cor_matrix_filtered, method = "color", type = "upper", 
           addCoef.col = "black", number.cex = 0.6,
           tl.cex = 0.8, tl.col = "black", order = "hclust",
           title = "Highly Correlated Features (r > 0.5)", mar=c(0,0,1,0))
} else {
  message("No sufficiently correlated features found.")
}


```

# Model Selection
Describe the chosen model
Why did we chose this model (proper explanation)

### Depends on the model i have chosen - One hot encoding? ###


# Training
explain how we train and why we train the way we train

```{r training}



```

# Testing
explain the used evaluation metrics, justify - why do we use the metrics we use

Then evaluate performance of the model

```{r testing}



```

# Explainability Analysis 
Describe the used explanation techniques
Run explainability techniques
Clear understanding of which features drive predictions and how they influence wage predictions. 

(permutation feature importance (PDP), Individual Conditional Expections (ICE), Lime, SHAP)

```{r explainability techniques}


```

# Prediction of our wages
Use model to predict our wages

```{r prediction of our wages}


```

# Conclusion and Reflections